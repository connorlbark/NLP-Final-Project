{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Lyric Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Lyric: at first i was afraid i was petrified thinking i couldnt live without you by my ...\n",
      "Num Lyrics: 4717\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('billboard_lyrics_1964-2015.csv', encoding='latin-1')\n",
    "df = df.drop_duplicates(['Song', 'Artist']).dropna(subset=['Lyrics']) # remove duplicate songs\n",
    "df = df.sample(frac=1, random_state=1234) # randomize order\n",
    "lyrics = list(df['Lyrics'].astype(str).copy())\n",
    "del df # save some memory\n",
    "lyrics = [lyric.strip() for lyric in lyrics]\n",
    "print('Example Lyric:', lyrics[0][:80]+'...')\n",
    "print('Num Lyrics:', len(lyrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "char_tokenizer = Tokenizer(char_level=True)\n",
    "\n",
    "char_tokenizer.fit_on_texts(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PaddedBatchDataset element_spec=(TensorSpec(shape=(None, None, 52), dtype=tf.int32, name=None), TensorSpec(shape=(None, 52), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# generate one hotted data\n",
    "def char_data_generator(lyrics, tokenizer):\n",
    "    for lyric in lyrics:\n",
    "        char_representation = tokenizer.texts_to_sequences([lyric])[0]\n",
    "        one_hot_representation = to_categorical(char_representation, num_classes=len(tokenizer.word_index) + 1)\n",
    "        \n",
    "        for i in range(len(one_hot_representation)):\n",
    "            yield np.array(one_hot_representation[:i]), np.array(one_hot_representation[i])\n",
    "\n",
    "n_char_examples = sum([len(lyric) for lyric in lyrics])\n",
    "n_batched_char_examples = int(math.ceil(n_char_examples / 128))\n",
    "\n",
    "shuffle_buffer = 500\n",
    "batch_size = 128\n",
    "        \n",
    "char_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: char_data_generator(lyrics, char_tokenizer),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, len(char_tokenizer.word_index)+1), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(len(char_tokenizer.word_index)+1), dtype=tf.int32)))\n",
    "\n",
    "char_dataset = char_dataset.shuffle(shuffle_buffer)\n",
    "char_dataset = char_dataset.padded_batch(batch_size)\n",
    "\n",
    "char_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "def build_char_model(num_chars):\n",
    "    return Sequential(\n",
    "        [\n",
    "            Input(shape=(None, num_chars)),\n",
    "            LSTM(128),\n",
    "            Dense(num_chars, activation=\"softmax\"),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "char_model = build_char_model(len(char_tokenizer.word_index)+1)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "char_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_model.fit(\n",
    "    char_dataset, \n",
    "    steps_per_epoch = n_batched_char_examples, \n",
    "    epochs = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Level Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word Embeddings\n",
    "Word usage is very different in lyrics than in everyday language. Thus, training our own word embeddings is likely to produce better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_size = 200\n",
    "\n",
    "model = Word2Vec(sentences=[lyric.split() for lyric in lyrics], vector_size=embedding_size, window=5, min_count=2, workers=4)\n",
    "model.wv.save_word2vec_format('lyric_embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 18668\n"
     ]
    }
   ],
   "source": [
    "print('Vocab size {}'.format(len(model.wv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "\n",
    "word_tokenizer.fit_on_texts(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "KeyError: \"Key ' ' not present\"\nTraceback (most recent call last):\n\n  File \"/Users/connorbarker/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/Users/connorbarker/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/Users/connorbarker/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-37-a51657bb651d>\", line 9, in word_embedding_data_generator\n    embeddings = [np.array(model.wv[word]) for word in lyric]\n\n  File \"<ipython-input-37-a51657bb651d>\", line 9, in <listcomp>\n    embeddings = [np.array(model.wv[word]) for word in lyric]\n\n  File \"/Users/connorbarker/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\", line 395, in __getitem__\n    return self.get_vector(key_or_keys)\n\n  File \"/Users/connorbarker/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\", line 438, in get_vector\n    index = self.get_index(key)\n\n  File \"/Users/connorbarker/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\", line 412, in get_index\n    raise KeyError(f\"Key '{key}' not present\")\n\nKeyError: \"Key ' ' not present\"\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a51657bb651d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mword_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadded_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    820\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2921\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2922\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2923\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2924\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2925\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7185\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7186\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: KeyError: \"Key ' ' not present\"\nTraceback (most recent call last):\n\n  File \"/Users/connorbarker/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/Users/connorbarker/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/Users/connorbarker/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-37-a51657bb651d>\", line 9, in word_embedding_data_generator\n    embeddings = [np.array(model.wv[word]) for word in lyric]\n\n  File \"<ipython-input-37-a51657bb651d>\", line 9, in <listcomp>\n    embeddings = [np.array(model.wv[word]) for word in lyric]\n\n  File \"/Users/connorbarker/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\", line 395, in __getitem__\n    return self.get_vector(key_or_keys)\n\n  File \"/Users/connorbarker/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\", line 438, in get_vector\n    index = self.get_index(key)\n\n  File \"/Users/connorbarker/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\", line 412, in get_index\n    raise KeyError(f\"Key '{key}' not present\")\n\nKeyError: \"Key ' ' not present\"\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "# generate one hotted data\n",
    "def word_embedding_data_generator(lyrics, tokenizer):\n",
    "    unknown_word = np.zeros(len(word_tokenizer.word_index)+1, dtype=np.int32)\n",
    "    unknown_word[0] = 1\n",
    "    \n",
    "    for lyric in lyrics:\n",
    "        split_lyric = lyric.split()\n",
    "        \n",
    "        embeddings = [np.array(model.wv[word]) for word in lyric]\n",
    "        for i in range(len(1,embeddings)):\n",
    "            label = (np.array(tokenizer.word_index[split_lyric[i]]) \n",
    "                     if (split_lyric[i] in tokenizer.word_index) \n",
    "                     else unknown_word)\n",
    "            yield np.array(embeddings[:i]), label\n",
    "\n",
    "n_word_examples = sum([len(lyric.split())-1 for lyric in lyrics])\n",
    "n_batched_word_examples = int(math.ceil(n_word_examples / 128))\n",
    "\n",
    "shuffle_buffer = 500\n",
    "batch_size = 128\n",
    "        \n",
    "word_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: word_embedding_data_generator(lyrics, word_tokenizer),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, embedding_size), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(len(word_tokenizer.word_index)+1), dtype=tf.int32)))\n",
    "\n",
    "word_dataset = word_dataset.shuffle(shuffle_buffer)\n",
    "word_dataset = word_dataset.padded_batch(batch_size)\n",
    "\n",
    "list(word_dataset.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
